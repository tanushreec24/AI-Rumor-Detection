{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 959,
     "referenced_widgets": [
      "25bc07490a17459f90d7bd6bac58beff",
      "27fafe151be441aaad7e0986b1c02942",
      "e4d0587fa37845e7b3292b25a210236e",
      "75b90386e991494f98e715612fb48c6e",
      "a21e2c16bd554952bc62f1b2a540d50c",
      "0fe5991d634c4182b043ae27f66c7d77",
      "19a16a69994e4b00b3e615325be3deaf",
      "744bad96cc6344b29a45930626275490",
      "a6a2e4a485514760be69e1110b970e27",
      "9763c109ee5840b79dbdbddf5dca740a",
      "74c0057841ee4c8892943bc13eef71e4",
      "44dd511572b24960ace5f69e377e80a4",
      "abb0d7c55cb24739904b61e40f37dfc8",
      "b5d0bd81e7e54e31bb72a5cad6ea33c0",
      "45f55d4910514f66b01f8f73edc3d562",
      "2939d87a5cf84532be2cfb24cf0fe228",
      "cc31b49415044d3da63b3eccabaf2eb8",
      "9c8a67cdfb9a4742a421d4011aa186e0",
      "22a092c115444e1daa6a0dcaf5140098",
      "5ecdcd6292e74a12894199f6afd9dc1d",
      "1cc337ab97004617946227b5f544e062",
      "bbca00d9b0de4470831218ae59def082",
      "2f5b612b1e2f44b1a334c8028ad10944",
      "fd1dd3be0901425b8d5c0c30f3a5f28b",
      "d6a806d79dd549bbaade6417d00bfc49",
      "fc008dc4970241f388273a155ce842cd",
      "52d313568b2a42a18b31b4a87cf9aefc",
      "5544d2c254314caaa5f76d5848a99be0",
      "06accdc0dba04c3fbc7cd6e382313fd3",
      "a54d9e3bc2f145a8b33444e9e638d2f7",
      "207240cf6aad4027a207b83aa73ab2b9",
      "0ed5637422be4fe8a444f3040d3eb44f",
      "ce5731a6903a41ce8e5adec14878a2e3",
      "e5193f9b38744eb9bfb6cc67355eb640",
      "5cc0d3a787a049dfbfdab8a7edff4c8f",
      "f7d3a2f32890488bb0fda828c9c8d79d",
      "60f3a7e7e5c24f2e93d6d05d6f02142f",
      "f9ae83a544f740a2b5689e0bd6c215f4",
      "0f03d95436c5438ebddf0d4389b34c7e",
      "14aeb3d2905e49c98c13f2ba6c0301b3",
      "4d536163c8694b458585dc3df09bdbd4",
      "0afd0faae0be4c4090905aa8fdb672dc",
      "c7bbe9d2cbf744e588874c141a152219",
      "93ca15b7765445f08a8c7a1bde79f0eb",
      "e2b58cde19c64ac3a48309579a803e4c",
      "36cbed296bf24a2496fbc78c940ad05c",
      "2fc84dac61ef4f0180a2ecd73598fd85",
      "ee0636230555418f9e6f8cf22b1c4ab6",
      "c02ed48b701d44e98d3fe5a340084d26",
      "45e171a3ccdd472482999b95fe75bb08",
      "34682a16e629490983304ce9d462dbe7",
      "5e166c4a953a4f3da512b846794eab3c",
      "956577ae82c846ca8aa4cfd5c8419b00",
      "ac87773d63af435a88d9f34bd0d740ce",
      "6d26907f04c54a218adfc587425a9b62"
     ]
    },
    "id": "s0po1dQgcc4v",
    "outputId": "58ac599c-ffb9-4ce8-ce07-7e73794eda77"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25bc07490a17459f90d7bd6bac58beff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44dd511572b24960ace5f69e377e80a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f5b612b1e2f44b1a334c8028ad10944",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5193f9b38744eb9bfb6cc67355eb640",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2b58cde19c64ac3a48309579a803e4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting Sequential Training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9369' max='9369' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9369/9369 46:59, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.228800</td>\n",
       "      <td>0.236949</td>\n",
       "      <td>0.913764</td>\n",
       "      <td>0.824117</td>\n",
       "      <td>0.776130</td>\n",
       "      <td>0.799404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.201300</td>\n",
       "      <td>0.206149</td>\n",
       "      <td>0.923292</td>\n",
       "      <td>0.841588</td>\n",
       "      <td>0.805063</td>\n",
       "      <td>0.822921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.065000</td>\n",
       "      <td>0.292938</td>\n",
       "      <td>0.928257</td>\n",
       "      <td>0.857909</td>\n",
       "      <td>0.810127</td>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîπ Confusion Matrix:\n",
      "[[9266  458]\n",
      " [ 619 2146]]\n",
      "\n",
      "üîπ Confusion Matrix:\n",
      "[[9305  419]\n",
      " [ 539 2226]]\n",
      "\n",
      "üîπ Confusion Matrix:\n",
      "[[9353  371]\n",
      " [ 525 2240]]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='781' max='781' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [781/781 01:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîπ Confusion Matrix:\n",
      "[[9305  419]\n",
      " [ 539 2226]]\n",
      "\n",
      "üìä Sequential Processing Results:\n",
      "‚úÖ Accuracy: 0.9233\n",
      "‚úÖ Precision: 0.8416\n",
      "‚úÖ Recall: 0.8051\n",
      "‚úÖ F1-Score: 0.8229\n",
      "‚è≥ Total Execution Time (Sequential): 2821.93 sec\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
    "\n",
    "# ‚úÖ Disable Weights & Biases (W&B) logging\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "# 1Ô∏è‚É£ Load PHEME Dataset\n",
    "df = pd.read_csv(\"pheme-dataset-for-rumour-detection.csv\")  # Update with correct dataset path\n",
    "\n",
    "# ‚úÖ Drop NaN values in both text and labels\n",
    "df = df.dropna(subset=[\"Body\", \"Label\"])\n",
    "\n",
    "texts = df[\"Body\"].astype(str).tolist()  # Convert text to string format\n",
    "labels = df[\"Label\"].astype(int).tolist()  # Ensure labels are integers (0 or 1)\n",
    "\n",
    "# 2Ô∏è‚É£ Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(texts, labels, test_size=0.2, random_state=42, stratify=labels)\n",
    "\n",
    "# 3Ô∏è‚É£ Load BERT Tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Tokenization Function\n",
    "def encode_texts(texts, tokenizer, max_len=100):\n",
    "    return tokenizer(texts, padding=True, truncation=True, max_length=max_len, return_tensors=\"pt\")\n",
    "\n",
    "train_encodings = encode_texts(X_train, tokenizer)\n",
    "test_encodings = encode_texts(X_test, tokenizer)\n",
    "\n",
    "# 4Ô∏è‚É£ Convert Data to Torch Dataset\n",
    "class RumorDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = torch.tensor(labels)  # Convert labels to tensors\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item[\"labels\"] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "train_dataset = RumorDataset(train_encodings, y_train)\n",
    "test_dataset = RumorDataset(test_encodings, y_test)\n",
    "\n",
    "# 5Ô∏è‚É£ Load Pre-trained BERT Model\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "\n",
    "# 6Ô∏è‚É£ Training Arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    "    report_to=\"none\",  # ‚úÖ Disables W&B logging\n",
    ")\n",
    "\n",
    "# 7Ô∏è‚É£ Define Trainer Function\n",
    "def compute_metrics(pred):\n",
    "    logits = pred.predictions\n",
    "    preds = torch.argmax(torch.tensor(logits), axis=1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(pred.label_ids, preds, average='binary')\n",
    "    acc = accuracy_score(pred.label_ids, preds)\n",
    "    cm = confusion_matrix(pred.label_ids, preds)\n",
    "\n",
    "    print(\"\\nüîπ Confusion Matrix:\")\n",
    "    print(cm)  # Prints confusion matrix\n",
    "    return {\"accuracy\": acc, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# 8Ô∏è‚É£ Train Model & Measure Execution Time\n",
    "print(\"üöÄ Starting Sequential Training...\")\n",
    "start_time = time.time()\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "end_time = time.time()\n",
    "sequential_time = end_time - start_time\n",
    "\n",
    "# 9Ô∏è‚É£ Evaluate Model\n",
    "results = trainer.evaluate()\n",
    "\n",
    "# üîπ Print Performance Metrics & Execution Time\n",
    "print(\"\\nüìä Sequential Processing Results:\")\n",
    "print(f\"‚úÖ Accuracy: {results['eval_accuracy']:.4f}\")\n",
    "print(f\"‚úÖ Precision: {results['eval_precision']:.4f}\")\n",
    "print(f\"‚úÖ Recall: {results['eval_recall']:.4f}\")\n",
    "print(f\"‚úÖ F1-Score: {results['eval_f1']:.4f}\")\n",
    "print(f\"‚è≥ Total Execution Time (Sequential): {sequential_time:.2f} sec\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
